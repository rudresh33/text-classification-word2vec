# Text Classification: Dense Word Embeddings vs. Sparse Representations

[cite_start]This repository contains the codebase for the IMC-601: Introduction to Data Science project[cite: 2]. It evaluates text classification performance by comparing a traditional sparse feature model against a modern dense embedding approach.

[cite_start]**Author:** Rudresh Achari [cite: 1]
[cite_start]**Program:** MSc Integrated Third Year, Second Semester, Batch: 2023 [cite: 3]

## Dataset
* [cite_start]**Name:** 20 Newsgroups Dataset [cite: 8]
* [cite_start]**Training Size:** 11,314 documents [cite: 25]
* [cite_start]**Testing Size:** 7,532 documents [cite: 27]

## Methodology
The experiment contrasts two text classification pipelines:

### 1. Baseline Method (TF-IDF + Multinomial Naive Bayes)
* [cite_start]**Feature Extraction:** TF-IDF Vectorizer (max features: 10,000, min_df: 2, max_df: 0.95)[cite: 20, 21, 23].
* [cite_start]**Classifier:** Multinomial Naive Bayes (Laplace smoothing alpha=1.0)[cite: 49, 50].

### 2. Proposed Method (Word2Vec + Feed-Forward Neural Network)
* [cite_start]**Feature Extraction:** Custom Word2Vec model trained on the corpus (Skip-gram, dimension: 100, window size: 5, min frequency: 2, epochs: 10)[cite: 10, 12, 13, 14, 55]. [cite_start]Document vectors were generated by taking the element-wise mean of Word2Vec vectors[cite: 60].
* [cite_start]**Classifier:** Shallow Feed-Forward Neural Network (FNN)[cite: 65].
    * [cite_start]Input Layer: 100 neurons [cite: 67]
    * [cite_start]Dropout Layer: 0.5 dropout rate [cite: 68]
    * [cite_start]Hidden Layer 1: 128 neurons (ReLU activation) [cite: 69]
    * [cite_start]Hidden Layer 2: 64 neurons (ReLU activation) [cite: 70]
    * [cite_start]Output Layer: 20 neurons (Softmax activation) [cite: 71]
    * [cite_start]Optimizer: Adam (Learning rate: 0.001) [cite: 72]

## Results
| Model Architecture | Accuracy | Macro F1-Score |
| :--- | :--- | :--- |
| Proposed Method (Word2Vec + FNN) | 55.38% | 52.07% |
| Baseline Method (TF-IDF + MNB) | 66.28% | 63.42% |

## Conclusion
Contrary to initial expectations, the traditional MNB baseline outperformed the proposed FNN model. [cite_start]The shift from a sparse, context-blind feature space to a dense, semantic-aware feature space [cite: 88] successfully captured word-level similarities. [cite_start]However, the use of Document Vector Averaging as the feature engineering technique loses crucial word order information[cite: 93]. [cite_start]For example, "dog bites man" and "man bites dog" produce identical document vectors despite opposite meanings[cite: 94]. This demonstrates that for long-form text classification, sparse keyword frequencies can still provide stronger discriminative signals to linear models than aggressively averaged dense vectors.
